dataset:
  enable_constant: true
  file: datasets/openorca_large_subset_011.jsonl
  max_input_tokens: 128
  max_output_tokens: 128
  max_queries: 3000
  max_sequence_tokens: 16000
  min_input_tokens: 128
  min_output_tokens: 128
extra_metadata:
  replicas: 1
load_options:
  concurrency: 8
  duration: 300
  type: constant
output:
  dir: ./output/
  file: output-input_tokens128-output_tokens128-batch1-pod8.json
  format: json
plugin: openai_plugin
plugin_options:
  constant_output_tokens: 128
  custom_headers:
    Host: huggingface-ipex-llama2.default.svc.cluster.local
  endpoint: /openai/v1/chat/completions
  host: http://172.21.8.113:80
  model_name: llama2
  model_path: /media/shared_folder/AIGC-PVC/pvc-91cd0f04-8188-4e39-876e-e9133af61d2f/model-space/Llama-2-7b-chat-hf
  proxies: {}
  streaming: true
  timeout_sec: 300
  use_tls: false
storage:
  type: local
warmup: false
warmup_options:
  requests: 1
  timeout_sec: 60
