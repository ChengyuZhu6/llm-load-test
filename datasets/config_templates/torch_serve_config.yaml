output:
  format: "json" # Maybe add option for pickle?
  dir: "./output/"
  file: "output.json"
warmup: True
warmup_options:
  requests: 3
  timeout_sec: 60
storage: # TODO
  type: local
dataset:
  file: "datasets/openorca_large_subset_011.jsonl"
  max_queries: 3000
  min_input_tokens: 0
  max_input_tokens: 16000
  max_output_tokens: 128
  max_sequence_tokens: 32000
load_options:
  type: constant #Future options: loadgen, stair-step
  concurrency: 8
  duration: 300 # In seconds. Maybe in future support "100s" "10m", etc...
plugin: "torch_serve_plugin"
plugin_options:
  #interface: "grpc" # Some plugins like caikit-nlp-client should support grpc/http
  use_tls: False # Use True if querying an SSL grpc endpoint over https
  streaming: False
  model_name: "llama"
  model_path: "/media/shared_folder/AIGC-PVC/pvc-91cd0f04-8188-4e39-876e-e9133af61d2f/model-space/Llama-2-7b-hf/"
  host: "http://172.21.10.199"
  endpoint: "/v1/models/llama2-7b-ipex"
  api: "predict"
  custom_headers: {"Host": "llama2inferenceserving.default.svc.cluster.local"}
  proxies: {}
  timeout_sec: 300
extra_metadata:
  replicas: 1
