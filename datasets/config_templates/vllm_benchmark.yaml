output:
  format: "json" 
  name: "output"
  dir: "./output/"
  csv: "benchmark.csv"
llm_engine: "vllm"
model_name: "llama2"
model_path: "/media/shared_folder/AIGC-PVC/pvc-91cd0f04-8188-4e39-876e-e9133af61d2f/model-space/Llama-2-7b-chat-hf"
inference_deploy_yaml_path: "/media/shared_folder/med-alpaca/huggingface-vllm-ipex/vllm-ipex.yaml"
benchmark_config_path: "./config.yaml"
test_script_path: "./load_test.py"
output_tokens: [128]
input_tokens: [128, 256, 512, 1024, 2048]
batch_size: [1, 2, 4, 8, 16, 32, 64]
pod_num: [8]
duration: 300
enable_constant: true